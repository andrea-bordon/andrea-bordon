{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAtm50mH/DD0nnz5gvvyO8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea-bordon/andrea-bordon/blob/main/Product_embeddings_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwz2bcfk8PRF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from datetime import datetime\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders = (pd.read_csv('./data/order_products__train.csv')\n",
        "          ._append(pd.read_csv('./data/order_products__prior.csv'))\n",
        "         )\n",
        "products = pd.read_csv('./data/products.csv')"
      ],
      "metadata": {
        "id": "QkwPSvqBR5D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_cols = ['order_id','product_name']\n",
        "\n",
        "#downsample while I test the code for faster iteration on syntax. run full dataset before commit.\n",
        "sample_size = 1\n",
        "\n",
        "baskets = (orders\n",
        "           .merge(products,on='product_id',how='left')\n",
        "           .sample(frac=sample_size)\n",
        "          )[relevant_cols]\n",
        "\n",
        "#memory management on my local computer\n",
        "del([orders,products])"
      ],
      "metadata": {
        "id": "pjoMN7UcR_W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baskets.sort_values(['order_id']).head(20)"
      ],
      "metadata": {
        "id": "iMNfiHkVSEx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_items = baskets.product_name.nunique()\n",
        "embedding_size = np.floor(num_items**0.25).astype('int')\n",
        "print('''Let's use vectors of length {n} for {tokens} products'''.format(n=embedding_size, tokens = num_items))\n",
        "\n",
        "biggest_basket = np.max(baskets.groupby('order_id').product_name.nunique())\n",
        "print('''The biggest basket (window in our algorithm) will be {}'''.format(biggest_basket))"
      ],
      "metadata": {
        "id": "joQhgS5lSHuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_of_basket_lists = (baskets\n",
        "        .groupby('order_id')\n",
        "        .apply(lambda baskets :\n",
        "                baskets.product_name\n",
        "                .tolist()\n",
        "               )\n",
        "       )\n",
        "\n",
        "#memory management\n",
        "del(baskets)"
      ],
      "metadata": {
        "id": "oN0ZEw0mSOhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_of_basket_lists.head()\n",
        "print(len(df_of_basket_lists))"
      ],
      "metadata": {
        "id": "atQhJPM2SRkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(df_of_basket_lists, vector_size=embedding_size, window=biggest_basket)"
      ],
      "metadata": {
        "id": "GKrjq99QSUAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(word_u,word_v,model):\n",
        "    \"\"\"\n",
        "    Cosine similarity gets the similarity for two products and computes the similarity\n",
        "    between two embeddings in our word2vec model\n",
        "\n",
        "    Arguments:\n",
        "        u - numpy array of shape (n,)\n",
        "        v - numpy array of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine similarity between words u & v\n",
        "    \"\"\"\n",
        "    #get embeddings from gensim model\n",
        "    u = model.wv[word_u]\n",
        "    v = model.wv[word_v]\n",
        "\n",
        "    #compute similarity\n",
        "    dot = np.dot(u, v)\n",
        "    norm_u = np.sqrt(np.sum(u * u))\n",
        "    norm_v = np.sqrt(np.sum(v * v))\n",
        "    cosine_similarity = dot / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "id": "dqsx6GGlSV1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.index_to_key), sample)\n",
        "#            words = np.random.choice(list(model.vocab.keys()), sample)\n",
        "\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "mdfOX8_ISYMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('Chicken Fingers')"
      ],
      "metadata": {
        "id": "Zs6Ct_AeSb9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same aisle same category\n",
        "products_draw = ['Spicy Chicken Breast Patties', 'Gluten Free Crispy Battered Haddock',\n",
        "'Key West Pink Shrimp',\n",
        "'Potato Crunch Fish Fillets',\n",
        "'Classic Seasoning with Lemon Skillet Crisp Tilapia',\n",
        "'Mini Crispy Crabless Cakes',\n",
        "'Whole Grain Breaded Chicken Breast Chunks',\n",
        "'Chipotle Black Bean Burger',\n",
        "'Cracked Peppercorn Tilapia',\n",
        "'Angus Beef Meatballs',\n",
        "'Crispy Strips',\n",
        "'Chicken Fingers',\n",
        "'Patties, Beef, Quarter Pound',\n",
        "'Original Turkey Burgers Smoke Flavor Added',\n",
        "'Tortilla Crusted Tilapia',\n",
        "'Crispy Chicken',\n",
        "'Breaded Vegan Coconut Shrimp',\n",
        "'Breaded Chicken Patties',\n",
        "'Crab Cakes',\n",
        "'Breaded Nuggets Chicken Breast']"
      ],
      "metadata": {
        "id": "FJwFPE-nSeov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(model.wv, products_draw)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vst4zLuaSdwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}